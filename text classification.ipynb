{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset provided\n",
    "data = pd.read_csv(\"bbc-text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1557\n",
      "Test size: 668\n"
     ]
    }
   ],
   "source": [
    "## function to create \n",
    "\n",
    "train_size = int(len(data) * .7)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))\n",
    "\n",
    "def train_test_conversion(data, train_size):\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating and saving post-processed data\n",
    "\n",
    "train_cat, test_cat = train_test_conversion(data['category'], train_size)\n",
    "train_text, test_text = train_test_conversion(data['text'], train_size)\n",
    "\n",
    "\n",
    "convert_train = pd.concat([train_cat,train_text], axis=1)\n",
    "convert_train.to_csv(r'train.csv')\n",
    "\n",
    "convert_test = pd.concat([test_cat,test_text], axis=1)\n",
    "convert_test.to_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "## creating embeddings\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "\n",
    "f= open('glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word]= coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.'%len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text( doc, remove_stopwords=False):\n",
    "    # remove HTML\n",
    "    doc_text = BeautifulSoup(doc).get_text()\n",
    "    # remove non-letters\n",
    "    doc_text = re.sub(\"[^a-zA-Z]\",\" \", doc_text)\n",
    "    # remove multiple white spaces and trailing white spaces\n",
    "    doc_text = re.sub(\" +\",\" \",doc_text)\n",
    "    doc_text = doc_text.strip()\n",
    "    # convert words to lower case and split them\n",
    "    words = doc_text.lower().split()\n",
    "    # optionally remove stop words.\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text']\n",
    "labels = data['category']\n",
    "\n",
    "MAX_NB_WORDS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizeing vectorinzing and cleaning data\n",
    "\n",
    "processed_texts = [clean_text(t) for t in texts]\n",
    "vectorizer = CountVectorizer(max_features=MAX_NB_WORDS)\n",
    "vectorizer_fit = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "words  = vectorizer.get_feature_names()\n",
    "counts = vectorizer_fit.toarray().sum(axis=0)\n",
    "counts_words = zip(counts,words)\n",
    "#counts_words.sort(reverse=True)\n",
    "counts_words = sorted(counts_words, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining padding_sequence and One-hot encoder\n",
    "\n",
    "def pad_sequences(seq, maxlen):\n",
    "    if len(seq) >= maxlen:\n",
    "        return np.array(seq[-maxlen:]).astype('int32')\n",
    "    else:\n",
    "        return np.pad(seq, (maxlen - len(seq)%maxlen, 0), 'constant').astype('int32')\n",
    "    \n",
    "def one_hot(x):\n",
    "    return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "vocabulary = [str(w[1]) for w in counts_words]\n",
    "word_index = dict(zip(vocabulary, range(MAX_NB_WORDS)))\n",
    "\n",
    "sequences = []\n",
    "for doc in processed_texts:\n",
    "    sequence=[]\n",
    "    for word in doc.split():\n",
    "        if word not in word_index:\n",
    "            continue\n",
    "        sequence.append(word_index[word])\n",
    "    sequences.append(sequence)\n",
    "\n",
    "data = np.vstack([pad_sequences(s,MAX_SEQUENCE_LENGTH) for s in sequences])\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2225, 1000)\n",
      "Shape of label tensor: (1557,)\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data, labels, stratify=labels, test_size=0.3)\n",
    "\n",
    "#y_train = one_hot(np.asarray(y_train))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "    # prepare embedding matrix\n",
    "num_words = MAX_NB_WORDS\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_cat)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ## Function to build a model.\n",
    " ##  Accepts standard inputs: x, y and mode (TRAIN, test)\n",
    " ## Returns a ModelFnOps Object that will be passed to learn.Estimator\n",
    "\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features, [-1, MAX_SEQUENCE_LENGTH])\n",
    "\n",
    "    # embedding layer and look up\n",
    "    embeddings = tf.get_variable(name=\"embeddings\", shape=embedding_matrix.shape,\n",
    "                                 initializer=tf.constant_initializer(embedding_matrix), trainable=False)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_layer)\n",
    "\n",
    "    #  3 convolution, pooling and batch normalization layers\n",
    "    conv1 = tf.layers.conv1d(inputs=embed, filters=128,kernel_size=5, padding=\"VALID\", activation=tf.nn.relu)\n",
    "    # one could add l2 regularization as:\n",
    "    # conv1 = tf.layers.conv1d(inputs=embed, filters=128,kernel_size=5, padding=\"VALID\",\n",
    "    #     activation=tf.nn.relu, activity_regularizer=tf.contrib.layers.l2_regularizer(0.001))\n",
    "    pool1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=5, strides=5, padding=\"VALID\")\n",
    "    bn1 = tf.layers.batch_normalization(pool1)\n",
    "\n",
    "    conv2 = tf.layers.conv1d(inputs=bn1, filters=128, kernel_size=5, padding=\"VALID\", activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=5, strides=5, padding=\"VALID\")\n",
    "    bn2 = tf.layers.batch_normalization(pool2)\n",
    "\n",
    "    conv3 = tf.layers.conv1d(inputs=bn2, filters=128, kernel_size=5, padding=\"VALID\", activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=35, strides=35, padding=\"VALID\")\n",
    "    bn3 = tf.layers.batch_normalization(pool3)\n",
    "\n",
    "    # Dense Layer\n",
    "    bn3_flat = tf.reshape(bn3, [-1, 128])\n",
    "    dense = tf.layers.dense(inputs=bn3_flat, units=128, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.5, training=mode == learn.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=20)\n",
    "\n",
    "    # defining and keep tracking of loss and optimization\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    # here mode is defined train, test,\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=20)\n",
    "        loss = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=0.001,\n",
    "            optimizer=\"Adam\")\n",
    "\n",
    "    # Generate Predictions\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(\n",
    "            input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(\n",
    "            logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    # Return a ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "        mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020174CC6DD8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': './model_layer'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./model_layer\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.143201, step = 0\n",
      "INFO:tensorflow:global_step/sec: 18.9215\n",
      "INFO:tensorflow:loss = 0.15136775, step = 100 (5.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.114\n",
      "INFO:tensorflow:loss = 0.07645481, step = 200 (4.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.0931\n",
      "INFO:tensorflow:loss = 0.043905437, step = 300 (4.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.9468\n",
      "INFO:tensorflow:loss = 0.0018475717, step = 400 (5.013 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.0975\n",
      "INFO:tensorflow:loss = 0.00023931006, step = 500 (4.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.1113\n",
      "INFO:tensorflow:loss = 0.0048722643, step = 600 (4.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.8972\n",
      "INFO:tensorflow:loss = 0.00048400142, step = 700 (5.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.0407\n",
      "INFO:tensorflow:loss = 0.00075762597, step = 800 (4.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.0197\n",
      "INFO:tensorflow:loss = 0.00018464118, step = 900 (4.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.0249\n",
      "INFO:tensorflow:loss = 0.18223898, step = 1000 (4.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.8667\n",
      "INFO:tensorflow:loss = 0.0003788125, step = 1100 (5.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.0306\n",
      "INFO:tensorflow:loss = 0.00020812899, step = 1200 (4.992 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1250 into ./model_layer\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00017372264.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    ## implementing function of model cnn\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    text_classifier = learn.Estimator(\n",
    "        model_fn=cnn_model_fn, model_dir=\"./model_layer\")\n",
    "\n",
    "    # Train the model\n",
    "    text_classifier.fit(x=x_train,y=y_train,batch_size=128,steps=1250)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-d24e266f2dfb>:4: MetricSpec.__init__ (from tensorflow.contrib.learn.python.learn.metric_spec) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.estimator.EstimatorSpec.eval_metric_ops.\n",
      "WARNING:tensorflow:From <ipython-input-18-d24e266f2dfb>:8: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-18-d24e266f2dfb>:8: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Starting evaluation at 2019-07-05-04:28:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model_layer\\model.ckpt-1250\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-05-04:29:00\n",
      "INFO:tensorflow:Saving dict for global step 1250: accuracy = 0.9730539, global_step = 1250, loss = 0.15524441\n",
      "{'accuracy': 0.9730539, 'loss': 0.15524441, 'global_step': 1250}\n"
     ]
    }
   ],
   "source": [
    "# Configure the accuracy metric for evaluation\n",
    "\n",
    "metrics = {\"accuracy\":learn.MetricSpec(\n",
    "        metric_fn=tf.metrics.accuracy, prediction_key=\"classes\")}\n",
    "\n",
    "# Evaluate the model and print results\n",
    "\n",
    "eval_results = text_classifier.evaluate(x=x_test, y=y_test, metrics=metrics)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
